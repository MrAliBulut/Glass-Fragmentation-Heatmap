{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the Image**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "\n",
    "# Read the input image\n",
    "image = cv2.imread(\"Input/glass.jpg\")\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1) Cropping the image** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert image to binary format/Grayscale.\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Blur the grayscale image for noise reduction.\n",
    "gauss_blurred = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "\n",
    "# Detect the edges in the blurred image using the Canny edge detector.\n",
    "edged = cv2.Canny(gauss_blurred, 110, 200)\n",
    "\n",
    "# Create a Rectangle kernel for morphological dilation\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
    "\n",
    "# Dilate the edges to thicken them.\n",
    "mask = cv2.dilate(edged, kernel, iterations=5)\n",
    "\n",
    "# Find all of the contours in the dilated image\n",
    "contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "\n",
    "# Find the largest contour in the list of contours. We assume this is the edge surrounding the window.\n",
    "largest_contour_index = np.argmax([cv2.contourArea(c) for c in contours])\n",
    "largest_contour = contours[largest_contour_index]\n",
    "\n",
    "# Get the coordinates of the bounding rectangle of the largest contour.\n",
    "x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "\n",
    "# Crop the input image using the coordinates of the bounding rectangle.\n",
    "cropped_image = image[y:y+h, x:x+w]\n",
    "\n",
    "cv2.imwrite(\"Output/Cropped Image.png\", cropped_image)\n",
    "\n",
    "plt.imshow(cropped_image)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Center Circle Removal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the cropped image to grayscale\n",
    "cropped_image_gray= cv2.cvtColor(cropped_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Heavily blur the image to find center circle. This will make sure that the edges caused by fragmentation are removed by Canny that is implemented inside HoughCircles().\n",
    "cropped_image_gray = cv2.medianBlur(cropped_image_gray, 101)\n",
    "\n",
    "# Detect circles in the cropped grayscale image using the HoughCircles() function\n",
    "circles = cv2.HoughCircles(cropped_image_gray, cv2.HOUGH_GRADIENT, 1, cropped_image_gray.shape[0] / 1, param1=40, param2=30, minRadius=0, maxRadius=0)\n",
    "\n",
    "# First, check if there are any circles. If there are, then fill the largest circle in black. We assume that is the center circle.\n",
    "if circles is not None:\n",
    "    circles = np.uint16(np.around(circles))\n",
    "    for i in circles[0, :]:\n",
    "        center = (i[0], i[1])\n",
    "        radius = i[2]\n",
    "        cv2.circle(cropped_image, center, radius, (0, 0, 0), cv2.FILLED)\n",
    "\n",
    "cv2.imwrite(\"Output/After Circle Removal.png\", cropped_image)\n",
    "\n",
    "plt.imshow(cropped_image)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Background Removal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the cropped and center removed image to grayscale\n",
    "gray = cv2.cvtColor(cropped_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Detect edges. Here we want to detect as many fragmentation edges as possible. But we have to be wary of the pixels that are out of the window.\n",
    "edges = cv2.Canny(gray, threshold1=30, threshold2=100)\n",
    "\n",
    "# Dilate the fragmentation edges to thicken them.\n",
    "mask = cv2.dilate(edges, None, iterations=5)\n",
    "\n",
    "# Convert all the irrelevant pixels to black.\n",
    "result = cv2.bitwise_and(cropped_image, cropped_image, mask=mask)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "cv2.imwrite(\"Output/Black Background.png\",result)\n",
    "\n",
    "plt.imshow(result)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transparentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This process is here because the client wished us to implement it.\n",
    "\n",
    "# Convert the image to RGBA mode (for transparency)\n",
    "result = cv2.cvtColor(result, cv2.COLOR_BGR2BGRA)\n",
    "\n",
    "# Get the pixel data as a list of tuples (B, G, R, A)\n",
    "pixel_data = list(result.reshape(-1, 4))\n",
    "\n",
    "\n",
    "# If the sum of the r,g,b channels are lesser than 30, make its alpha channel 0. (fully transparent with alpha=0)\n",
    "new_pixel_data = [(r, g, b, 0) if sum((r, g, b)) <= 30 else (r, g, b, a) for (r, g, b, a) in pixel_data]\n",
    "\n",
    "# Reshape the pixel data back to the original image shape\n",
    "transparent_output = np.array(new_pixel_data, dtype=np.uint8).reshape(result.shape[0], result.shape[1], 4)\n",
    "\n",
    "cv2.imwrite(\"Output/Transparent Background.png\",transparent_output)\n",
    "\n",
    "plt.imshow(transparent_output)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HeatMap**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the colormap to the image (excluding transparent areas)\n",
    "colored_image = cv2.applyColorMap(transparent_output[:, :, 0], cv2.COLORMAP_JET)\n",
    "\n",
    "result_image = cv2.addWeighted(colored_image, 1, transparent_output[:, :, :3], 1, 0)\n",
    "\n",
    "gray = cv2.cvtColor(result_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "heatmap = cv2.applyColorMap(gray, cv2.COLORMAP_JET)\n",
    "heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGRA2RGBA)\n",
    "\n",
    "plt.imshow(heatmap)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import PIL library to remove blue background easily.\n",
    "from PIL import Image\n",
    "\n",
    "image_array = np.array(heatmap)\n",
    "\n",
    "# Define the target color (blue in this case) and the color threshold\n",
    "target_color = np.array([0, 0, 255, 255])  # RGBA color for blue (with alpha channel)\n",
    "color_threshold = 170  # Adjust this threshold as needed\n",
    "\n",
    "# Create a mask for the blue pixels based on the threshold\n",
    "blue_pixels = np.all(np.abs(image_array[:, :, :3] - target_color[:3]) <= color_threshold, axis=-1)\n",
    "\n",
    "# Set the blue pixels to be transparent (fully transparent with alpha=0)\n",
    "image_array[blue_pixels, 2] = 0\n",
    "\n",
    "# Create a new image from the modified array\n",
    "result_image = Image.fromarray(image_array)\n",
    "\n",
    "result_image.save('Output/Heatmapped Image.png')\n",
    "\n",
    "plt.imshow(result_image)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
